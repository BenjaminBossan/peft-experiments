{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed98c1f5-8829-4a3a-9ad6-0c9b879e1361",
   "metadata": {},
   "source": [
    "# Llama3 8b training using PEFT model with torchtune training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b1de2-5f69-45f4-84c6-63e2a58fee20",
   "metadata": {},
   "source": [
    "Hyper-parameters and data have been matched.\n",
    "\n",
    "torchtune uses torchao with 8bit and nf4 (?)\n",
    "\n",
    "bitsandbytes v0.43.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa212eb0-46b5-48cf-8e1a-de9c0f6b373e",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074893bb-d980-4394-9b2d-f912c34f46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from types import SimpleNamespace\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2002e01d-647e-4ce8-8fc3-d692a72ccbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torchtune import utils\n",
    "from torchtune.modules import get_cosine_schedule_with_warmup\n",
    "from torchtune.datasets import InstructDataset\n",
    "from torchtune.datasets._instruct import _get_component_from_path\n",
    "from torchtune.utils import padded_collate, get_memory_stats\n",
    "from torchtune.utils.metric_logging import DiskLogger\n",
    "from torchtune.models.llama3 import llama3_tokenizer\n",
    "from torchtune.modules.common_utils import reparametrize_as_dtype_state_dict_post_hook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f380886f-cec8-4ed7-bd9e-ec9cb95de9ae",
   "metadata": {},
   "source": [
    "## params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5449054f-62b8-493c-9021-176e2edbe7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16\n",
    "device = 0\n",
    "rank = 8\n",
    "alpha = 16\n",
    "target_modules = ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "weight_decay = 0.01\n",
    "dropout = 0.05\n",
    "lr = 0.0003\n",
    "shuffle = True\n",
    "batch_size = 2\n",
    "# dataset: torchtune.datasets.alpaca_cleaned_dataset\n",
    "gradient_accumulation_steps = 4\n",
    "total_epochs = 1\n",
    "max_seq_len = 512\n",
    "template = _get_component_from_path('torchtune.data.AlpacaInstructTemplate')\n",
    "log_every_n_steps = 1\n",
    "num_warmup_steps = 100\n",
    "max_steps_per_epoch = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096bd48-fb59-42de-aefe-5bcdb70a0769",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d9a6ce-7ff1-4bd7-83c4-1f15faaa4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a58a429-0bc8-4b6a-9288-1edfd184e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.expanduser(\"~/work/clones/torchtune/recipes/configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47390610-efb9-4971-8fb8-2918b4c0124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = llama3_tokenizer(os.path.join(base_path, \"Meta-Llama-3-8B-Instruct/original/tokenizer.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b62dc3c5-1d43-43c4-8837-db3ada395f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = InstructDataset(\n",
    "    tokenizer, 'yahma/alpaca-cleaned', train_on_input=True, max_seq_len=max_seq_len, split=\"train\", template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8e936c5-70af-42ac-a997-b6e426868bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = torch.utils.data.DistributedSampler(\n",
    "    ds,\n",
    "    num_replicas=1,\n",
    "    rank=0,\n",
    "    shuffle=shuffle,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b48550a-4502-4211-802f-dea7ce7c0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = False\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=ds,\n",
    "    sampler=sampler,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=(\n",
    "        partial(\n",
    "            padded_collate,\n",
    "            padding_idx=128004,\n",
    "            ignore_idx=-100,\n",
    "        )\n",
    "        if not packed\n",
    "        else None\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d9c35c1-867f-42ef-a490-f5510f92004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10c622a8-3dd3-41cd-9d3d-0ec30eca13e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526df2981059461d99ca13d4e473ad24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    torch_dtype=dtype,\n",
    "    device_map=device,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0fbf694-7961-4892-9902-01db1c2f27b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=rank,\n",
    "    lora_alpha=alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=dropout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75ee7d5b-40b2-42de-8f9e-5a251db6358c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config, autocast_adapter_dtype=False) # torchtune uses bf16\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9954f3d8-4fbe-457b-8f79-99addc8cc042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x793c05f0ab50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as in torchtune\n",
    "model._register_state_dict_hook(\n",
    "    partial(reparametrize_as_dtype_state_dict_post_hook, offload_to_cpu=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2d4e17d-2bcf-4cc2-93e8-32366f58111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=weight_decay, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c81c5e3b-4ab5-47ca-ae7b-e293e934f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeb5b567-d15e-47f6-bcd1-47b094c182e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_epochs=1, steps_per_epoch=6470, num_training_steps=6470\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(dataloader) // gradient_accumulation_steps\n",
    "num_training_steps = total_epochs * steps_per_epoch\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "print(f\"{total_epochs=}, {steps_per_epoch=}, {num_training_steps=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71492b7e-70cd-454c-a70b-bc0924a87afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /tmp/peft/llama3-8b-qlora-4bit/log_1724160813.txt\n"
     ]
    }
   ],
   "source": [
    "metric_logger = DiskLogger(\"/tmp/peft/llama3-8b-qlora-4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48779570-b219-4c85-84ef-b1cfe5f50466",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f6a43-0864-4903-974e-fc1524572cf3",
   "metadata": {},
   "source": [
    "### emulate \"self\" object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d7faef5-89e5-4768-8659-abbc75ede318",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = SimpleNamespace(\n",
    "    epochs_run=0,\n",
    "    global_step=0,\n",
    "    total_epochs=total_epochs,\n",
    "    _sampler=sampler,\n",
    "    _steps_per_epoch=steps_per_epoch,\n",
    "    _dataloader=dataloader,\n",
    "    _gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    max_steps_per_epoch=max_steps_per_epoch,\n",
    "    _device=torch.device(0),\n",
    "    _model=model,\n",
    "    _loss_fn=criterion,\n",
    "    _optimizer=optimizer,\n",
    "    _log_every_n_steps=log_every_n_steps,\n",
    "    _log_peak_memory_stats=True,\n",
    "    _lr_scheduler=lr_scheduler,\n",
    "    _metric_logger=metric_logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a63bc-aaf7-4c9d-941f-fdc6c2d5695a",
   "metadata": {},
   "source": [
    "### train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20851476-a96e-4185-a8ff-865c0fc35f4f",
   "metadata": {},
   "source": [
    "Copied from:\n",
    "\n",
    "https://github.com/pytorch/torchtune/blob/bc6b7e9132542e2f6d47d28fab338d42f9b2242d/recipes/lora_dpo_single_device.py#L479\n",
    "\n",
    "Interrupted early, as not much movement anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0d3cf07-7bf3-4930-8ebd-2239243aabea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e13affabb9747b2ad4edc310e80d6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Exception ignored in: <function tqdm.__del__ at 0x793d585d3f60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vinh/anaconda3/envs/peft/lib/python3.11/site-packages/tqdm/std.py\", line 1147, in __del__\n",
      "    def __del__(self):\n",
      "\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "running_loss = 0\n",
    "num_tokens = 0\n",
    "\n",
    "for curr_epoch in range(self.epochs_run, self.total_epochs):\n",
    "    # Update the sampler to ensure data is correctly shuffled across epochs\n",
    "    # in case shuffle is True\n",
    "    self._sampler.set_epoch(curr_epoch)\n",
    "\n",
    "    pbar = tqdm(total=self._steps_per_epoch)\n",
    "    for idx, batch in enumerate(self._dataloader):\n",
    "        if (\n",
    "            self.max_steps_per_epoch is not None\n",
    "            and (idx // self._gradient_accumulation_steps)\n",
    "            == self.max_steps_per_epoch\n",
    "        ):\n",
    "            break\n",
    "\n",
    "        # Both are shape [b, s]\n",
    "        tokens, labels = batch[\"tokens\"], batch[\"labels\"]\n",
    "        # Get the attention mask and position ids from the dataset if they\n",
    "        # exist. Currently, only sample packing in PackedDataset returns these\n",
    "        mask = batch.get(\"mask\", None)  # shape [b, s, s]\n",
    "        input_pos = batch.get(\"input_pos\", None)  # shape [b, s]\n",
    "\n",
    "        tokens = tokens.to(self._device)\n",
    "        num_tokens += tokens.numel()\n",
    "        labels = labels.to(self._device)\n",
    "        mask = mask.to(self._device) if mask is not None else None\n",
    "        input_pos = (\n",
    "            input_pos.to(self._device) if input_pos is not None else None\n",
    "        )\n",
    "\n",
    "        # uncomment to use transformers\n",
    "        loss = self._model(tokens, attention_mask=mask, labels=labels).loss\n",
    "\n",
    "        loss = loss / self._gradient_accumulation_steps\n",
    "        running_loss += loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Step with optimizer\n",
    "        if (idx + 1) % self._gradient_accumulation_steps == 0:\n",
    "            self._optimizer.step()\n",
    "            self._optimizer.zero_grad(set_to_none=True)\n",
    "            self._lr_scheduler.step()\n",
    "            # Update the number of steps when the weights are updated\n",
    "            self.global_step += 1\n",
    "\n",
    "            loss_to_log = running_loss.item()\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(\n",
    "                f\"{curr_epoch + 1}|{self.global_step}|Loss: {loss_to_log}\"\n",
    "            )\n",
    "\n",
    "            # Log per-step metrics\n",
    "            if self.global_step % self._log_every_n_steps == 0:\n",
    "                time_per_step = time.perf_counter() - t0\n",
    "                log_dict = {\n",
    "                    \"loss\": loss_to_log,\n",
    "                    \"lr\": self._optimizer.param_groups[0][\"lr\"],\n",
    "                    \"tokens_per_second_per_gpu\": num_tokens / time_per_step,\n",
    "                }\n",
    "                if (\n",
    "                    self._device.type == \"cuda\"\n",
    "                    and self._log_peak_memory_stats\n",
    "                ):\n",
    "                    log_dict.update(\n",
    "                        utils.get_memory_stats(device=self._device)\n",
    "                    )\n",
    "                self._metric_logger.log_dict(\n",
    "                    log_dict,\n",
    "                    step=self.global_step,\n",
    "                )\n",
    "\n",
    "            # Reset running stats for the next step\n",
    "            running_loss = 0\n",
    "            num_tokens = 0\n",
    "            t0 = time.perf_counter()\n",
    "\n",
    "        # Step the profiler\n",
    "        # Note we are stepping each batch, which might not include optimizer step in the trace\n",
    "        # if the schedule cycle doesn't align with gradient accumulation.\n",
    "        #prof.step()\n",
    "\n",
    "    self.epochs_run += 1\n",
    "    #self.save_checkpoint(epoch=curr_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
