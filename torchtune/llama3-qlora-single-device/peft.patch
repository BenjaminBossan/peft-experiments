diff --git a/recipes/lora_finetune_single_device.py b/recipes/lora_finetune_single_device.py
index d827076e..a597c5c2 100644
--- a/recipes/lora_finetune_single_device.py
+++ b/recipes/lora_finetune_single_device.py
@@ -338,6 +338,8 @@ class LoRAFinetuneRecipeSingleDevice(FTRecipeInterface):
         lora_weights_state_dict: Optional[Dict[str, Any]] = None,
     ) -> nn.Module:
         with utils.set_default_dtype(self._dtype), self._device:
+            cfg_model.lora_attn_modules = []
+            cfg_model.apply_lora_to_mlp = False
             model = config.instantiate(cfg_model)
 
         self._lora_rank = cfg_model.lora_rank
@@ -356,27 +358,13 @@ class LoRAFinetuneRecipeSingleDevice(FTRecipeInterface):
         base_missing, base_unexpected = model.load_state_dict(
             base_model_state_dict, strict=False
         )
-        if lora_weights_state_dict:
-            lora_missing, lora_unexpected = model.load_state_dict(
-                lora_weights_state_dict, strict=False
-            )
-        else:
-            lora_missing, lora_unexpected = None, None
-        validate_missing_and_unexpected_for_lora(
-            lora_attn_modules=self._lora_attn_modules,
-            apply_lora_to_mlp=self._apply_lora_to_mlp,
-            apply_lora_to_output=self._apply_lora_to_output,
-            base_missing=base_missing,
-            base_unexpected=base_unexpected,
-            lora_missing=lora_missing,
-            lora_unexpected=lora_unexpected,
-        )
-        # Validate model adapter params were loaded in with the expected dtype
-        # TODO (rohan-varma): Further validation to ensure the appropriate base params
-        # are NF4 vs bf16 based on the quantization config.
-        utils.validate_expected_param_dtype(
-            self.adapter_params.items(), dtype=self._dtype
+
+        from peft import get_peft_model, LoraConfig
+        conf = LoraConfig(
+            r=cfg_model.lora_rank, lora_alpha=cfg_model.lora_alpha,
+            target_modules=['q_proj', 'v_proj', 'k_proj', 'output_proj', 'w1', 'w2', 'w3'],
         )
+        model = get_peft_model(model, conf, autocast_adapter_dtype=False)
 
         log.info(f"Model is initialized with precision {self._dtype}.")
         # Compile model, if enabled.
@@ -387,6 +375,12 @@ class LoRAFinetuneRecipeSingleDevice(FTRecipeInterface):
         if self._device.type == "cuda":
             memory_stats = utils.get_memory_stats(device=self._device)
             utils.log_memory_stats(memory_stats)
+
+        for name, module in model.named_modules():
+            if ("lora_A" in name) or ("lora_B" in name):
+                if len(list(module.children())) == 0:
+                    module.requires_grad_(True)
+
         return model
 
     def _setup_optimizer(
diff --git a/torchtune/models/llama3/_component_builders.py b/torchtune/models/llama3/_component_builders.py
index 155bb5f9..a410119f 100644
--- a/torchtune/models/llama3/_component_builders.py
+++ b/torchtune/models/llama3/_component_builders.py
@@ -304,10 +304,6 @@ def lora_llama3_self_attention(
     Raises:
         ValueError: If lora_modules arg is an empty list
     """
-    if not lora_modules:
-        raise ValueError(
-            f"Must pass one or more of {LORA_ATTN_MODULES} as lora_modules"
-        )
 
     head_dim = embed_dim // num_heads
     num_kv_heads = num_kv_heads if num_kv_heads else num_heads
